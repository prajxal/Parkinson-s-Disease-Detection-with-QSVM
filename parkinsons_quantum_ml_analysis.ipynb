{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Quantum vs Classical Machine Learning for Parkinson's Disease Detection\n",
        "\n",
        "This notebook compares classical machine learning models with quantum kernel methods for Parkinson's disease classification using voice features.\n",
        "\n",
        "## Dataset Information\n",
        "\n",
        "**UCI Parkinsons Dataset (Voice Features)**:\n",
        "- **Source**: UCI ML Repository - https://archive.ics.uci.edu/ml/datasets/Parkinsons\n",
        "- **Instances**: 197 voice recordings from 31 people (23 with PD, 8 healthy)\n",
        "- **Features**: 22 voice measurements (fundamental frequency, jitter, shimmer, etc.)\n",
        "- **Target**: Binary classification (0=healthy, 1=Parkinson's disease)\n",
        "\n",
        "**Parkinsons Telemonitoring Dataset**:\n",
        "- **Source**: UCI ML Repository - https://archive.ics.uci.edu/ml/datasets/Parkinsons+Telemonitoring\n",
        "- **Instances**: 5,875 recordings from 42 people with early-stage PD\n",
        "- **Features**: 16 voice measures + demographic info\n",
        "- **Target**: Regression (UPDRS scores)\n",
        "\n",
        "## Clinical Limitations\n",
        "\n",
        "⚠️ **Important**: This is a research/educational tool. For clinical diagnosis:\n",
        "- Requires validation on larger, diverse populations\n",
        "- Should be used in conjunction with clinical assessment\n",
        "- Consider demographic and environmental factors\n",
        "- Voice features alone may not be sufficient for diagnosis\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Dependencies and Configuration\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# CONFIGURABLE HYPERPARAMETERS\n",
        "# ============================================================================\n",
        "\n",
        "# Quantum Kernel Parameters\n",
        "N_QUBITS = 4  # Number of qubits for quantum feature map (adjust based on PCA components)\n",
        "N_LAYERS = 2  # Number of layers in the quantum feature map\n",
        "USE_HARDWARE = False  # Set to True if you have IBM Quantum credentials\n",
        "IBM_DEVICE = 'ibmq_qasm_simulator'  # Change to actual device name if using hardware\n",
        "\n",
        "# Classical Model Parameters\n",
        "N_SPLITS = 5  # Number of folds for cross-validation\n",
        "RANDOM_STATE = 42\n",
        "TEST_SIZE = 0.2  # For final train/test split\n",
        "\n",
        "# PCA Parameters\n",
        "PCA_VARIANCE_THRESHOLD = 0.95  # Retain 95% variance (will determine actual n_components)\n",
        "\n",
        "# Bootstrap Parameters\n",
        "N_BOOTSTRAP = 1000  # Number of bootstrap samples for confidence intervals\n",
        "CONFIDENCE_LEVEL = 0.95  # 95% confidence intervals\n",
        "\n",
        "# File Paths\n",
        "DATA_PATH_VOICE = 'parkinsons/parkinsons.data'\n",
        "DATA_PATH_TELEMON = 'parkinsons/telemonitoring/parkinsons_updrs.data'\n",
        "OUTPUT_DIR = 'outputs'\n",
        "\n",
        "# Create output directory\n",
        "import os\n",
        "os.makedirs(OUTPUT_DIR, exist_ok=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# INSTALL DEPENDENCIES (uncomment if needed)\n",
        "# ============================================================================\n",
        "# !pip install numpy pandas scikit-learn matplotlib seaborn pennylane qiskit qiskit-ibm-provider scipy joblib tqdm\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# IMPORTS\n",
        "# ============================================================================\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.model_selection import StratifiedKFold, train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import (\n",
        "    roc_auc_score, roc_curve, confusion_matrix, classification_report,\n",
        "    accuracy_score, precision_score, recall_score, f1_score\n",
        ")\n",
        "from scipy import stats\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Quantum computing imports\n",
        "import pennylane as qml\n",
        "from pennylane import numpy as pnp\n",
        "\n",
        "# Set style\n",
        "plt.style.use('seaborn-v0_8-darkgrid')\n",
        "sns.set_palette(\"husl\")\n",
        "\n",
        "print(\"✓ All imports successful\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Data Loading and Preprocessing\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 2.1 Load UCI Parkinsons Voice Dataset\n",
        "\n",
        "This dataset contains voice measurements from 31 people (23 with PD, 8 healthy). Each person has multiple recordings.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load the main Parkinsons voice dataset\n",
        "df_voice = pd.read_csv(DATA_PATH_VOICE)\n",
        "\n",
        "print(f\"Dataset shape: {df_voice.shape}\")\n",
        "print(f\"\\nColumn names:\\n{df_voice.columns.tolist()}\")\n",
        "print(f\"\\nFirst few rows:\")\n",
        "df_voice.head()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Extract subject IDs from the 'name' column\n",
        "# Format: phon_R01_S01_1 -> subject ID is S01\n",
        "df_voice['subject_id'] = df_voice['name'].str.extract(r'S(\\d+)')[0].astype(int)\n",
        "\n",
        "# Separate features and target\n",
        "feature_cols = [col for col in df_voice.columns if col not in ['name', 'status', 'subject_id']]\n",
        "X = df_voice[feature_cols].values\n",
        "y = df_voice['status'].values\n",
        "subject_ids = df_voice['subject_id'].values\n",
        "\n",
        "print(f\"Number of features: {len(feature_cols)}\")\n",
        "print(f\"Feature names: {feature_cols}\")\n",
        "print(f\"\\nClass distribution:\")\n",
        "print(f\"  Healthy (0): {np.sum(y == 0)} samples\")\n",
        "print(f\"  Parkinson's (1): {np.sum(y == 1)} samples\")\n",
        "print(f\"\\nNumber of unique subjects: {len(np.unique(subject_ids))}\")\n",
        "print(f\"Subjects per class:\")\n",
        "for subject in np.unique(subject_ids):\n",
        "    subject_mask = subject_ids == subject\n",
        "    subject_class = y[subject_mask][0]\n",
        "    print(f\"  Subject {subject}: {np.sum(subject_mask)} recordings, class={subject_class}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 2.2 Load Telemonitoring Dataset (Optional - for additional analysis)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load telemonitoring dataset (for reference/exploration)\n",
        "df_telemon = pd.read_csv(DATA_PATH_TELEMON)\n",
        "\n",
        "print(f\"Telemonitoring dataset shape: {df_telemon.shape}\")\n",
        "print(f\"\\nNumber of unique subjects: {df_telemon['subject#'].nunique()}\")\n",
        "print(f\"\\nColumns: {df_telemon.columns.tolist()}\")\n",
        "df_telemon.head()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Exploratory Data Analysis (EDA)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 3.1 Summary Statistics\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Summary statistics\n",
        "df_features = df_voice[feature_cols]\n",
        "summary_stats = df_features.describe()\n",
        "print(\"Summary Statistics:\")\n",
        "print(summary_stats)\n",
        "\n",
        "# Save to file\n",
        "summary_stats.to_csv(f\"{OUTPUT_DIR}/summary_statistics.csv\")\n",
        "print(f\"\\n✓ Saved summary statistics to {OUTPUT_DIR}/summary_statistics.csv\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 3.2 Class Balance Analysis\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Class distribution\n",
        "class_counts = pd.Series(y).value_counts().sort_index()\n",
        "class_props = pd.Series(y).value_counts(normalize=True).sort_index()\n",
        "\n",
        "fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
        "\n",
        "# Count plot\n",
        "axes[0].bar(['Healthy (0)', \"Parkinson's (1)\"], class_counts.values, color=['green', 'red'], alpha=0.7)\n",
        "axes[0].set_title('Class Distribution (Counts)', fontsize=14, fontweight='bold')\n",
        "axes[0].set_ylabel('Number of Samples')\n",
        "for i, v in enumerate(class_counts.values):\n",
        "    axes[0].text(i, v + 2, str(v), ha='center', fontweight='bold')\n",
        "\n",
        "# Proportion plot\n",
        "axes[1].bar(['Healthy (0)', \"Parkinson's (1)\"], class_props.values * 100, color=['green', 'red'], alpha=0.7)\n",
        "axes[1].set_title('Class Distribution (Percentages)', fontsize=14, fontweight='bold')\n",
        "axes[1].set_ylabel('Percentage (%)')\n",
        "for i, v in enumerate(class_props.values):\n",
        "    axes[1].text(i, v * 100 + 1, f'{v*100:.1f}%', ha='center', fontweight='bold')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig(f'{OUTPUT_DIR}/class_distribution.png', dpi=300, bbox_inches='tight')\n",
        "plt.show()\n",
        "\n",
        "print(f\"Class balance ratio: {class_counts[0] / class_counts[1]:.2f}:1 (healthy:PD)\")\n",
        "print(f\"Dataset is {'balanced' if 0.8 < class_props[0] < 0.2 or 0.8 < class_props[1] < 0.2 else 'imbalanced'}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 3.3 Subject Count Analysis\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Analyze recordings per subject\n",
        "subject_analysis = df_voice.groupby('subject_id').agg({\n",
        "    'status': 'first',\n",
        "    'name': 'count'\n",
        "}).rename(columns={'name': 'n_recordings'})\n",
        "\n",
        "print(\"Recordings per subject:\")\n",
        "print(subject_analysis)\n",
        "\n",
        "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
        "\n",
        "# Histogram of recordings per subject\n",
        "axes[0].hist(subject_analysis['n_recordings'], bins=range(1, subject_analysis['n_recordings'].max() + 2), \n",
        "             edgecolor='black', alpha=0.7)\n",
        "axes[0].set_xlabel('Number of Recordings per Subject')\n",
        "axes[0].set_ylabel('Number of Subjects')\n",
        "axes[0].set_title('Distribution of Recordings per Subject', fontsize=14, fontweight='bold')\n",
        "axes[0].grid(True, alpha=0.3)\n",
        "\n",
        "# Recordings by class\n",
        "class_0_subjects = subject_analysis[subject_analysis['status'] == 0]\n",
        "class_1_subjects = subject_analysis[subject_analysis['status'] == 1]\n",
        "\n",
        "axes[1].bar(['Healthy', \"Parkinson's\"], \n",
        "            [class_0_subjects['n_recordings'].mean(), class_1_subjects['n_recordings'].mean()],\n",
        "            color=['green', 'red'], alpha=0.7, edgecolor='black')\n",
        "axes[1].set_ylabel('Average Recordings per Subject')\n",
        "axes[1].set_title('Average Recordings per Subject by Class', fontsize=14, fontweight='bold')\n",
        "axes[1].grid(True, alpha=0.3, axis='y')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig(f'{OUTPUT_DIR}/subject_analysis.png', dpi=300, bbox_inches='tight')\n",
        "plt.show()\n",
        "\n",
        "print(f\"\\nAverage recordings per subject: {subject_analysis['n_recordings'].mean():.1f}\")\n",
        "print(f\"Min recordings: {subject_analysis['n_recordings'].min()}\")\n",
        "print(f\"Max recordings: {subject_analysis['n_recordings'].max()}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 3.4 Feature Distribution Histograms\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Plot histograms for all features, colored by class\n",
        "n_features = len(feature_cols)\n",
        "n_cols = 4\n",
        "n_rows = (n_features + n_cols - 1) // n_cols\n",
        "\n",
        "fig, axes = plt.subplots(n_rows, n_cols, figsize=(16, 4 * n_rows))\n",
        "axes = axes.flatten()\n",
        "\n",
        "for idx, feature in enumerate(feature_cols):\n",
        "    ax = axes[idx]\n",
        "    \n",
        "    # Plot histograms for each class\n",
        "    healthy_data = df_voice[df_voice['status'] == 0][feature]\n",
        "    pd_data = df_voice[df_voice['status'] == 1][feature]\n",
        "    \n",
        "    ax.hist(healthy_data, bins=20, alpha=0.6, label='Healthy', color='green', edgecolor='black')\n",
        "    ax.hist(pd_data, bins=20, alpha=0.6, label=\"Parkinson's\", color='red', edgecolor='black')\n",
        "    \n",
        "    ax.set_title(feature, fontsize=10, fontweight='bold')\n",
        "    ax.set_xlabel('Value')\n",
        "    ax.set_ylabel('Frequency')\n",
        "    ax.legend(fontsize=8)\n",
        "    ax.grid(True, alpha=0.3)\n",
        "\n",
        "# Hide unused subplots\n",
        "for idx in range(n_features, len(axes)):\n",
        "    axes[idx].axis('off')\n",
        "\n",
        "plt.suptitle('Feature Distributions by Class', fontsize=16, fontweight='bold', y=0.995)\n",
        "plt.tight_layout()\n",
        "plt.savefig(f'{OUTPUT_DIR}/feature_histograms.png', dpi=300, bbox_inches='tight')\n",
        "plt.show()\n",
        "\n",
        "print(f\"✓ Saved feature histograms to {OUTPUT_DIR}/feature_histograms.png\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 3.5 Correlation Matrix\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Compute correlation matrix\n",
        "corr_matrix = df_features.corr()\n",
        "\n",
        "plt.figure(figsize=(14, 12))\n",
        "mask = np.triu(np.ones_like(corr_matrix, dtype=bool))  # Mask upper triangle\n",
        "sns.heatmap(corr_matrix, mask=mask, annot=False, cmap='coolwarm', center=0,\n",
        "            square=True, linewidths=0.5, cbar_kws={\"shrink\": 0.8}, fmt='.2f')\n",
        "plt.title('Feature Correlation Matrix', fontsize=16, fontweight='bold', pad=20)\n",
        "plt.tight_layout()\n",
        "plt.savefig(f'{OUTPUT_DIR}/correlation_matrix.png', dpi=300, bbox_inches='tight')\n",
        "plt.show()\n",
        "\n",
        "print(f\"✓ Saved correlation matrix to {OUTPUT_DIR}/correlation_matrix.png\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Data Preprocessing for Modeling\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 4.1 Standardization and PCA\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Standardize features\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "# Apply PCA\n",
        "pca = PCA(n_components=PCA_VARIANCE_THRESHOLD)  # Retain specified variance\n",
        "X_pca = pca.fit_transform(X_scaled)\n",
        "\n",
        "n_components_actual = X_pca.shape[1]\n",
        "variance_explained = np.sum(pca.explained_variance_ratio_)\n",
        "\n",
        "print(f\"Original number of features: {X.shape[1]}\")\n",
        "print(f\"Number of PCA components (retaining {PCA_VARIANCE_THRESHOLD*100}% variance): {n_components_actual}\")\n",
        "print(f\"Actual variance explained: {variance_explained*100:.2f}%\")\n",
        "print(f\"\\nExplained variance per component:\")\n",
        "for i, var in enumerate(pca.explained_variance_ratio_[:10]):  # Show first 10\n",
        "    print(f\"  Component {i+1}: {var*100:.2f}%\")\n",
        "\n",
        "# Adjust N_QUBITS if needed\n",
        "if n_components_actual < N_QUBITS:\n",
        "    print(f\"\\n⚠️  Warning: N_QUBITS ({N_QUBITS}) > n_components ({n_components_actual}). Setting N_QUBITS = {n_components_actual}\")\n",
        "    N_QUBITS = n_components_actual\n",
        "elif n_components_actual > N_QUBITS:\n",
        "    # Use only first N_QUBITS components for quantum kernel\n",
        "    X_pca_quantum = X_pca[:, :N_QUBITS]\n",
        "    print(f\"\\nUsing first {N_QUBITS} PCA components for quantum kernel\")\n",
        "else:\n",
        "    X_pca_quantum = X_pca\n",
        "\n",
        "# Plot explained variance\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(range(1, len(pca.explained_variance_ratio_) + 1), \n",
        "         np.cumsum(pca.explained_variance_ratio_), 'bo-', linewidth=2, markersize=8)\n",
        "plt.axhline(y=PCA_VARIANCE_THRESHOLD, color='r', linestyle='--', label=f'{PCA_VARIANCE_THRESHOLD*100}% variance threshold')\n",
        "plt.xlabel('Number of Components', fontsize=12)\n",
        "plt.ylabel('Cumulative Explained Variance', fontsize=12)\n",
        "plt.title('PCA Explained Variance', fontsize=14, fontweight='bold')\n",
        "plt.grid(True, alpha=0.3)\n",
        "plt.legend()\n",
        "plt.tight_layout()\n",
        "plt.savefig(f'{OUTPUT_DIR}/pca_variance.png', dpi=300, bbox_inches='tight')\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 4.2 Subject-wise Train/Test Split\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Function for subject-wise splitting to avoid data leakage\n",
        "def subject_wise_split(X, y, subject_ids, test_size=0.2, random_state=42):\n",
        "    \"\"\"\n",
        "    Split data such that all recordings from a subject are in the same fold.\n",
        "    This prevents data leakage.\n",
        "    \"\"\"\n",
        "    unique_subjects = np.unique(subject_ids)\n",
        "    np.random.seed(random_state)\n",
        "    np.random.shuffle(unique_subjects)\n",
        "    \n",
        "    n_test_subjects = int(len(unique_subjects) * test_size)\n",
        "    test_subjects = unique_subjects[:n_test_subjects]\n",
        "    train_subjects = unique_subjects[n_test_subjects:]\n",
        "    \n",
        "    train_mask = np.isin(subject_ids, train_subjects)\n",
        "    test_mask = np.isin(subject_ids, test_subjects)\n",
        "    \n",
        "    return train_mask, test_mask\n",
        "\n",
        "# Create initial train/test split\n",
        "train_mask, test_mask = subject_wise_split(X_pca, y, subject_ids, test_size=TEST_SIZE, random_state=RANDOM_STATE)\n",
        "\n",
        "X_train_full = X_pca[train_mask]\n",
        "X_test = X_pca[test_mask]\n",
        "y_train_full = y[train_mask]\n",
        "y_test = y[test_mask]\n",
        "subject_ids_train = subject_ids[train_mask]\n",
        "subject_ids_test = subject_ids[test_mask]\n",
        "\n",
        "print(f\"Training set: {len(X_train_full)} samples from {len(np.unique(subject_ids_train))} subjects\")\n",
        "print(f\"Test set: {len(X_test)} samples from {len(np.unique(subject_ids_test))} subjects\")\n",
        "print(f\"\\nTraining class distribution: {np.bincount(y_train_full)}\")\n",
        "print(f\"Test class distribution: {np.bincount(y_test)}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Classical Machine Learning Baselines\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 5.1 Subject-wise Cross-Validation Function\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def subject_wise_cv(X, y, subject_ids, n_splits=5, random_state=42):\n",
        "    \"\"\"\n",
        "    Generate subject-wise cross-validation splits.\n",
        "    Ensures all recordings from a subject are in the same fold.\n",
        "    \"\"\"\n",
        "    unique_subjects = np.unique(subject_ids)\n",
        "    subject_labels = np.array([np.where(unique_subjects == sid)[0][0] for sid in subject_ids])\n",
        "    \n",
        "    # Use StratifiedKFold on subjects (not samples)\n",
        "    # Get class label for each subject\n",
        "    subject_classes = np.array([y[subject_ids == sid][0] for sid in unique_subjects])\n",
        "    \n",
        "    skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=random_state)\n",
        "    \n",
        "    for train_subj_idx, val_subj_idx in skf.split(unique_subjects, subject_classes):\n",
        "        train_subjects = unique_subjects[train_subj_idx]\n",
        "        val_subjects = unique_subjects[val_subj_idx]\n",
        "        \n",
        "        train_mask = np.isin(subject_ids, train_subjects)\n",
        "        val_mask = np.isin(subject_ids, val_subjects)\n",
        "        \n",
        "        yield train_mask, val_mask\n",
        "\n",
        "print(\"✓ Subject-wise CV function defined\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 5.2 Train Classical Models\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Initialize models\n",
        "models = {\n",
        "    'Logistic Regression': LogisticRegression(random_state=RANDOM_STATE, max_iter=1000),\n",
        "    'Random Forest': RandomForestClassifier(n_estimators=100, random_state=RANDOM_STATE, n_jobs=-1),\n",
        "    'SVM (RBF)': SVC(kernel='rbf', probability=True, random_state=RANDOM_STATE)\n",
        "}\n",
        "\n",
        "# Store results\n",
        "classical_results = {name: {'auc_scores': [], 'acc_scores': [], 'sens_scores': [], 'spec_scores': []} \n",
        "                     for name in models.keys()}\n",
        "\n",
        "print(\"Training classical models with subject-wise cross-validation...\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "for model_name, model in models.items():\n",
        "    print(f\"\\n{model_name}:\")\n",
        "    \n",
        "    for fold, (train_idx, val_idx) in enumerate(subject_wise_cv(X_train_full, y_train_full, \n",
        "                                                                 subject_ids_train, \n",
        "                                                                 n_splits=N_SPLITS, \n",
        "                                                                 random_state=RANDOM_STATE)):\n",
        "        X_train_cv = X_train_full[train_idx]\n",
        "        X_val_cv = X_train_full[val_idx]\n",
        "        y_train_cv = y_train_full[train_idx]\n",
        "        y_val_cv = y_train_full[val_idx]\n",
        "        \n",
        "        # Train model\n",
        "        model.fit(X_train_cv, y_train_cv)\n",
        "        \n",
        "        # Predictions\n",
        "        y_pred = model.predict(X_val_cv)\n",
        "        y_pred_proba = model.predict_proba(X_val_cv)[:, 1]\n",
        "        \n",
        "        # Metrics\n",
        "        auc = roc_auc_score(y_val_cv, y_pred_proba)\n",
        "        acc = accuracy_score(y_val_cv, y_pred)\n",
        "        \n",
        "        # Sensitivity (recall) and Specificity\n",
        "        tn, fp, fn, tp = confusion_matrix(y_val_cv, y_pred).ravel()\n",
        "        sens = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
        "        spec = tn / (tn + fp) if (tn + fp) > 0 else 0\n",
        "        \n",
        "        classical_results[model_name]['auc_scores'].append(auc)\n",
        "        classical_results[model_name]['acc_scores'].append(acc)\n",
        "        classical_results[model_name]['sens_scores'].append(sens)\n",
        "        classical_results[model_name]['spec_scores'].append(spec)\n",
        "        \n",
        "        print(f\"  Fold {fold+1}: AUC={auc:.3f}, Acc={acc:.3f}, Sens={sens:.3f}, Spec={spec:.3f}\")\n",
        "    \n",
        "    # Print average results\n",
        "    avg_auc = np.mean(classical_results[model_name]['auc_scores'])\n",
        "    avg_acc = np.mean(classical_results[model_name]['acc_scores'])\n",
        "    avg_sens = np.mean(classical_results[model_name]['sens_scores'])\n",
        "    avg_spec = np.mean(classical_results[model_name]['spec_scores'])\n",
        "    \n",
        "    print(f\"  Average: AUC={avg_auc:.3f}, Acc={avg_acc:.3f}, Sens={avg_sens:.3f}, Spec={avg_spec:.3f}\")\n",
        "\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"✓ Classical models trained\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Quantum Kernel SVM Implementation\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 6.1 Quantum Feature Map Setup\n",
        "\n",
        "We'll use an angle-encoding quantum feature map that encodes classical features into quantum states using rotation gates.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Configure PennyLane device\n",
        "if USE_HARDWARE:\n",
        "    # For IBM Quantum hardware (requires credentials)\n",
        "    # from qiskit import IBMQ\n",
        "    # IBMQ.load_account()\n",
        "    # dev = qml.device('qiskit.ibmq', wires=N_QUBITS, backend=IBM_DEVICE)\n",
        "    print(\"⚠️  Hardware mode not fully configured. Using simulator.\")\n",
        "    dev = qml.device('default.qubit', wires=N_QUBITS)\n",
        "else:\n",
        "    # Use local simulator\n",
        "    dev = qml.device('default.qubit', wires=N_QUBITS)\n",
        "\n",
        "print(f\"Using device: {dev.name}\")\n",
        "print(f\"Number of qubits: {N_QUBITS}\")\n",
        "print(f\"Number of layers: {N_LAYERS}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Define quantum feature map with angle encoding\n",
        "@qml.qnode(dev)\n",
        "def quantum_feature_map(x, params):\n",
        "    \"\"\"\n",
        "    Quantum feature map using angle encoding.\n",
        "    \n",
        "    Args:\n",
        "        x: Input features (normalized to [0, π])\n",
        "        params: Variational parameters for entangling layers\n",
        "    \n",
        "    Returns:\n",
        "        Quantum state measurement\n",
        "    \"\"\"\n",
        "    # Angle encoding: encode features as rotation angles\n",
        "    for i in range(N_QUBITS):\n",
        "        qml.RY(x[i], wires=i)\n",
        "    \n",
        "    # Entangling layers with variational parameters\n",
        "    for layer in range(N_LAYERS):\n",
        "        # Entangling gates\n",
        "        for i in range(N_QUBITS - 1):\n",
        "            qml.CNOT(wires=[i, i + 1])\n",
        "        \n",
        "        # Variational rotations\n",
        "        param_idx = layer * N_QUBITS\n",
        "        for i in range(N_QUBITS):\n",
        "            if param_idx + i < len(params):\n",
        "                qml.RY(params[param_idx + i], wires=i)\n",
        "    \n",
        "    # Measure in computational basis\n",
        "    return qml.state()\n",
        "\n",
        "# Alternative: simpler feature map for kernel computation\n",
        "@qml.qnode(dev)\n",
        "def quantum_kernel_circuit(x1, x2):\n",
        "    \"\"\"\n",
        "    Quantum kernel circuit that computes |<φ(x1)|φ(x2)>|²\n",
        "    This is the fidelity between two quantum states.\n",
        "    Uses the SWAP test approach for kernel computation.\n",
        "    \"\"\"\n",
        "    # Encode first data point\n",
        "    for i in range(N_QUBITS):\n",
        "        qml.RY(x1[i], wires=i)\n",
        "    \n",
        "    # Adjoint encoding of second data point (reverse order)\n",
        "    for i in range(N_QUBITS - 1, -1, -1):\n",
        "        qml.RY(-x2[i], wires=i)\n",
        "    \n",
        "    # Return probability of |0...0> state (fidelity)\n",
        "    return qml.probs(wires=range(N_QUBITS))[0]\n",
        "\n",
        "print(\"✓ Quantum feature map defined\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 6.2 Quantum Kernel Matrix Computation\n",
        "\n",
        "The quantum kernel is computed as the overlap between quantum states: K(x_i, x_j) = |<φ(x_i)|φ(x_j)>|²\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def compute_quantum_kernel(X1, X2=None):\n",
        "    \"\"\"\n",
        "    Compute quantum kernel matrix.\n",
        "    \n",
        "    Args:\n",
        "        X1: First set of data points (n_samples1, n_features)\n",
        "        X2: Second set of data points (n_samples2, n_features). If None, X2 = X1.\n",
        "    \n",
        "    Returns:\n",
        "        Kernel matrix of shape (n_samples1, n_samples2)\n",
        "    \"\"\"\n",
        "    if X2 is None:\n",
        "        X2 = X1\n",
        "    \n",
        "    # Normalize features to [0, π] for angle encoding\n",
        "    # Assuming features are already normalized (from StandardScaler)\n",
        "    X1_norm = (X1 - X1.min(axis=0)) / (X1.max(axis=0) - X1.min(axis=0) + 1e-10) * np.pi\n",
        "    X2_norm = (X2 - X2.min(axis=0)) / (X2.max(axis=0) - X2.min(axis=0) + 1e-10) * np.pi\n",
        "    \n",
        "    # Ensure we only use first N_QUBITS features\n",
        "    if X1_norm.shape[1] > N_QUBITS:\n",
        "        X1_norm = X1_norm[:, :N_QUBITS]\n",
        "    if X2_norm.shape[1] > N_QUBITS:\n",
        "        X2_norm = X2_norm[:, :N_QUBITS]\n",
        "    \n",
        "    n1 = X1_norm.shape[0]\n",
        "    n2 = X2_norm.shape[0]\n",
        "    kernel_matrix = np.zeros((n1, n2))\n",
        "    \n",
        "    print(f\"Computing quantum kernel matrix ({n1} x {n2})...\")\n",
        "    from tqdm import tqdm\n",
        "    \n",
        "    for i in tqdm(range(n1), desc=\"Computing kernel\"):\n",
        "        for j in range(n2):\n",
        "            # Compute overlap using quantum circuit\n",
        "            # Kernel value is |<φ(x1)|φ(x2)>|² = probability of |0...0> state\n",
        "            kernel_value = quantum_kernel_circuit(X1_norm[i], X2_norm[j])\n",
        "            kernel_matrix[i, j] = float(kernel_value)\n",
        "    \n",
        "    return kernel_matrix\n",
        "\n",
        "print(\"✓ Quantum kernel computation function defined\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 6.3 Train Quantum Kernel SVM with Cross-Validation\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Prepare quantum data (use first N_QUBITS PCA components)\n",
        "X_train_quantum = X_train_full[:, :N_QUBITS] if X_train_full.shape[1] >= N_QUBITS else X_train_full\n",
        "X_test_quantum = X_test[:, :N_QUBITS] if X_test.shape[1] >= N_QUBITS else X_test\n",
        "\n",
        "# Store quantum results\n",
        "quantum_results = {'auc_scores': [], 'acc_scores': [], 'sens_scores': [], 'spec_scores': []}\n",
        "\n",
        "print(\"Training Quantum Kernel SVM with subject-wise cross-validation...\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "for fold, (train_idx, val_idx) in enumerate(subject_wise_cv(X_train_quantum, y_train_full, \n",
        "                                                             subject_ids_train, \n",
        "                                                             n_splits=N_SPLITS, \n",
        "                                                             random_state=RANDOM_STATE)):\n",
        "    print(f\"\\nFold {fold+1}/{N_SPLITS}:\")\n",
        "    \n",
        "    X_train_cv = X_train_quantum[train_idx]\n",
        "    X_val_cv = X_train_quantum[val_idx]\n",
        "    y_train_cv = y_train_full[train_idx]\n",
        "    y_val_cv = y_train_full[val_idx]\n",
        "    \n",
        "    # Compute quantum kernel matrices\n",
        "    print(\"  Computing training kernel matrix...\")\n",
        "    K_train = compute_quantum_kernel(X_train_cv)\n",
        "    \n",
        "    print(\"  Computing validation kernel matrix...\")\n",
        "    K_val = compute_quantum_kernel(X_train_cv, X_val_cv)\n",
        "    \n",
        "    # Train SVM with precomputed kernel\n",
        "    svm_quantum = SVC(kernel='precomputed', probability=True, random_state=RANDOM_STATE)\n",
        "    svm_quantum.fit(K_train, y_train_cv)\n",
        "    \n",
        "    # Predictions\n",
        "    y_pred = svm_quantum.predict(K_val)\n",
        "    y_pred_proba = svm_quantum.predict_proba(K_val)[:, 1]\n",
        "    \n",
        "    # Metrics\n",
        "    auc = roc_auc_score(y_val_cv, y_pred_proba)\n",
        "    acc = accuracy_score(y_val_cv, y_pred)\n",
        "    \n",
        "    # Sensitivity and Specificity\n",
        "    tn, fp, fn, tp = confusion_matrix(y_val_cv, y_pred).ravel()\n",
        "    sens = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
        "    spec = tn / (tn + fp) if (tn + fp) > 0 else 0\n",
        "    \n",
        "    quantum_results['auc_scores'].append(auc)\n",
        "    quantum_results['acc_scores'].append(acc)\n",
        "    quantum_results['sens_scores'].append(sens)\n",
        "    quantum_results['spec_scores'].append(spec)\n",
        "    \n",
        "    print(f\"  Results: AUC={auc:.3f}, Acc={acc:.3f}, Sens={sens:.3f}, Spec={spec:.3f}\")\n",
        "\n",
        "# Print average results\n",
        "avg_auc = np.mean(quantum_results['auc_scores'])\n",
        "avg_acc = np.mean(quantum_results['acc_scores'])\n",
        "avg_sens = np.mean(quantum_results['sens_scores'])\n",
        "avg_spec = np.mean(quantum_results['spec_scores'])\n",
        "\n",
        "print(f\"\\n{'=' * 60}\")\n",
        "print(f\"Quantum Kernel SVM Average Results:\")\n",
        "print(f\"  AUC={avg_auc:.3f}, Acc={avg_acc:.3f}, Sens={avg_sens:.3f}, Spec={avg_spec:.3f}\")\n",
        "print(\"=\" * 60)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Results Comparison and Visualization\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 7.1 Bootstrap Confidence Intervals\n",
        "\n",
        "We use bootstrap resampling to compute confidence intervals for all metrics.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def bootstrap_ci(data, n_bootstrap=1000, confidence_level=0.95):\n",
        "    \"\"\"\n",
        "    Compute bootstrap confidence intervals.\n",
        "    \n",
        "    Args:\n",
        "        data: Array of metric values\n",
        "        n_bootstrap: Number of bootstrap samples\n",
        "        confidence_level: Confidence level (e.g., 0.95 for 95% CI)\n",
        "    \n",
        "    Returns:\n",
        "        (mean, lower_bound, upper_bound)\n",
        "    \"\"\"\n",
        "    bootstrap_samples = []\n",
        "    n = len(data)\n",
        "    \n",
        "    for _ in range(n_bootstrap):\n",
        "        # Resample with replacement\n",
        "        bootstrap_sample = np.random.choice(data, size=n, replace=True)\n",
        "        bootstrap_samples.append(np.mean(bootstrap_sample))\n",
        "    \n",
        "    bootstrap_samples = np.array(bootstrap_samples)\n",
        "    alpha = 1 - confidence_level\n",
        "    lower = np.percentile(bootstrap_samples, 100 * alpha / 2)\n",
        "    upper = np.percentile(bootstrap_samples, 100 * (1 - alpha / 2))\n",
        "    mean = np.mean(data)\n",
        "    \n",
        "    return mean, lower, upper\n",
        "\n",
        "# Compute bootstrap CIs for all models\n",
        "all_results = {}\n",
        "for model_name in classical_results.keys():\n",
        "    all_results[model_name] = {}\n",
        "    for metric in ['auc_scores', 'acc_scores', 'sens_scores', 'spec_scores']:\n",
        "        mean, lower, upper = bootstrap_ci(classical_results[model_name][metric], \n",
        "                                          n_bootstrap=N_BOOTSTRAP, \n",
        "                                          confidence_level=CONFIDENCE_LEVEL)\n",
        "        all_results[model_name][metric] = {'mean': mean, 'lower': lower, 'upper': upper}\n",
        "\n",
        "# Quantum results\n",
        "all_results['Quantum Kernel SVM'] = {}\n",
        "for metric in ['auc_scores', 'acc_scores', 'sens_scores', 'spec_scores']:\n",
        "    mean, lower, upper = bootstrap_ci(quantum_results[metric], \n",
        "                                      n_bootstrap=N_BOOTSTRAP, \n",
        "                                      confidence_level=CONFIDENCE_LEVEL)\n",
        "    all_results['Quantum Kernel SVM'][metric] = {'mean': mean, 'lower': lower, 'upper': upper}\n",
        "\n",
        "print(\"✓ Bootstrap confidence intervals computed\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 7.2 Results Table\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create results table\n",
        "results_table = []\n",
        "\n",
        "for model_name in all_results.keys():\n",
        "    row = {\n",
        "        'Model': model_name,\n",
        "        'ROC-AUC': f\"{all_results[model_name]['auc_scores']['mean']:.3f} \"\n",
        "                   f\"({all_results[model_name]['auc_scores']['lower']:.3f}-{all_results[model_name]['auc_scores']['upper']:.3f})\",\n",
        "        'Accuracy': f\"{all_results[model_name]['acc_scores']['mean']:.3f} \"\n",
        "                   f\"({all_results[model_name]['acc_scores']['lower']:.3f}-{all_results[model_name]['acc_scores']['upper']:.3f})\",\n",
        "        'Sensitivity': f\"{all_results[model_name]['sens_scores']['mean']:.3f} \"\n",
        "                      f\"({all_results[model_name]['sens_scores']['lower']:.3f}-{all_results[model_name]['sens_scores']['upper']:.3f})\",\n",
        "        'Specificity': f\"{all_results[model_name]['spec_scores']['mean']:.3f} \"\n",
        "                      f\"({all_results[model_name]['spec_scores']['lower']:.3f}-{all_results[model_name]['spec_scores']['upper']:.3f})\"\n",
        "    }\n",
        "    results_table.append(row)\n",
        "\n",
        "df_results = pd.DataFrame(results_table)\n",
        "print(\"Results Summary (with 95% Bootstrap Confidence Intervals):\")\n",
        "print(\"=\" * 100)\n",
        "print(df_results.to_string(index=False))\n",
        "print(\"=\" * 100)\n",
        "\n",
        "# Save to CSV\n",
        "df_results.to_csv(f'{OUTPUT_DIR}/results_summary.csv', index=False)\n",
        "print(f\"\\n✓ Saved results to {OUTPUT_DIR}/results_summary.csv\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 7.3 Visualization: Metric Comparison\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Plot comparison of metrics\n",
        "metrics = ['auc_scores', 'acc_scores', 'sens_scores', 'spec_scores']\n",
        "metric_names = ['ROC-AUC', 'Accuracy', 'Sensitivity', 'Specificity']\n",
        "\n",
        "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
        "axes = axes.flatten()\n",
        "\n",
        "for idx, (metric, metric_name) in enumerate(zip(metrics, metric_names)):\n",
        "    ax = axes[idx]\n",
        "    \n",
        "    model_names = list(all_results.keys())\n",
        "    means = [all_results[name][metric]['mean'] for name in model_names]\n",
        "    lowers = [all_results[name][metric]['lower'] for name in model_names]\n",
        "    uppers = [all_results[name][metric]['upper'] for name in model_names]\n",
        "    errors = [[m - l for m, l in zip(means, lowers)], \n",
        "              [u - m for u, m in zip(uppers, means)]]\n",
        "    \n",
        "    x_pos = np.arange(len(model_names))\n",
        "    bars = ax.bar(x_pos, means, yerr=errors, capsize=5, alpha=0.7, edgecolor='black')\n",
        "    \n",
        "    # Color quantum model differently\n",
        "    for i, (bar, name) in enumerate(zip(bars, model_names)):\n",
        "        if 'Quantum' in name:\n",
        "            bar.set_color('purple')\n",
        "        else:\n",
        "            bar.set_color(plt.cm.tab10(i))\n",
        "    \n",
        "    ax.set_xlabel('Model', fontsize=11)\n",
        "    ax.set_ylabel(metric_name, fontsize=11)\n",
        "    ax.set_title(f'{metric_name} Comparison', fontsize=12, fontweight='bold')\n",
        "    ax.set_xticks(x_pos)\n",
        "    ax.set_xticklabels(model_names, rotation=45, ha='right')\n",
        "    ax.set_ylim([0, 1.1])\n",
        "    ax.grid(True, alpha=0.3, axis='y')\n",
        "    \n",
        "    # Add value labels on bars\n",
        "    for i, (mean, lower, upper) in enumerate(zip(means, lowers, uppers)):\n",
        "        ax.text(i, mean + (upper - mean) + 0.02, f'{mean:.3f}', \n",
        "               ha='center', fontsize=9, fontweight='bold')\n",
        "\n",
        "plt.suptitle('Model Performance Comparison', fontsize=16, fontweight='bold', y=0.995)\n",
        "plt.tight_layout()\n",
        "plt.savefig(f'{OUTPUT_DIR}/metrics_comparison.png', dpi=300, bbox_inches='tight')\n",
        "plt.show()\n",
        "\n",
        "print(f\"✓ Saved metrics comparison to {OUTPUT_DIR}/metrics_comparison.png\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 7.4 ROC Curves\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Train final models on full training set and evaluate on test set\n",
        "print(\"Training final models on full training set...\")\n",
        "\n",
        "# Classical models\n",
        "final_classical_models = {}\n",
        "for model_name, model in models.items():\n",
        "    model.fit(X_train_full, y_train_full)\n",
        "    final_classical_models[model_name] = model\n",
        "\n",
        "# Quantum kernel SVM\n",
        "print(\"Computing quantum kernel for final model...\")\n",
        "K_train_final = compute_quantum_kernel(X_train_quantum)\n",
        "K_test_final = compute_quantum_kernel(X_train_quantum, X_test_quantum)\n",
        "\n",
        "svm_quantum_final = SVC(kernel='precomputed', probability=True, random_state=RANDOM_STATE)\n",
        "svm_quantum_final.fit(K_train_final, y_train_full)\n",
        "\n",
        "# Compute ROC curves\n",
        "plt.figure(figsize=(10, 8))\n",
        "\n",
        "# Classical models\n",
        "for model_name, model in final_classical_models.items():\n",
        "    y_pred_proba = model.predict_proba(X_test)[:, 1]\n",
        "    fpr, tpr, _ = roc_curve(y_test, y_pred_proba)\n",
        "    auc = roc_auc_score(y_test, y_pred_proba)\n",
        "    plt.plot(fpr, tpr, label=f'{model_name} (AUC = {auc:.3f})', linewidth=2)\n",
        "\n",
        "# Quantum model\n",
        "y_pred_proba_quantum = svm_quantum_final.predict_proba(K_test_final)[:, 1]\n",
        "fpr_q, tpr_q, _ = roc_curve(y_test, y_pred_proba_quantum)\n",
        "auc_q = roc_auc_score(y_test, y_pred_proba_quantum)\n",
        "plt.plot(fpr_q, tpr_q, label=f'Quantum Kernel SVM (AUC = {auc_q:.3f})', \n",
        "         linewidth=2, linestyle='--', color='purple')\n",
        "\n",
        "# Diagonal line (random classifier)\n",
        "plt.plot([0, 1], [0, 1], 'k--', label='Random Classifier (AUC = 0.500)', linewidth=1, alpha=0.5)\n",
        "\n",
        "plt.xlabel('False Positive Rate', fontsize=12)\n",
        "plt.ylabel('True Positive Rate', fontsize=12)\n",
        "plt.title('ROC Curves Comparison', fontsize=14, fontweight='bold')\n",
        "plt.legend(loc='lower right', fontsize=10)\n",
        "plt.grid(True, alpha=0.3)\n",
        "plt.tight_layout()\n",
        "plt.savefig(f'{OUTPUT_DIR}/roc_curves.png', dpi=300, bbox_inches='tight')\n",
        "plt.show()\n",
        "\n",
        "print(f\"✓ Saved ROC curves to {OUTPUT_DIR}/roc_curves.png\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 7.5 Confusion Matrices\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Plot confusion matrices for all models\n",
        "n_models = len(final_classical_models) + 1\n",
        "n_cols = 2\n",
        "n_rows = (n_models + n_cols - 1) // n_cols\n",
        "\n",
        "fig, axes = plt.subplots(n_rows, n_cols, figsize=(12, 5 * n_rows))\n",
        "axes = axes.flatten()\n",
        "\n",
        "model_idx = 0\n",
        "\n",
        "# Classical models\n",
        "for model_name, model in final_classical_models.items():\n",
        "    ax = axes[model_idx]\n",
        "    y_pred = model.predict(X_test)\n",
        "    cm = confusion_matrix(y_test, y_pred)\n",
        "    \n",
        "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=ax, \n",
        "                xticklabels=['Healthy', \"Parkinson's\"], \n",
        "                yticklabels=['Healthy', \"Parkinson's\"])\n",
        "    ax.set_title(f'{model_name}', fontsize=12, fontweight='bold')\n",
        "    ax.set_ylabel('True Label', fontsize=10)\n",
        "    ax.set_xlabel('Predicted Label', fontsize=10)\n",
        "    model_idx += 1\n",
        "\n",
        "# Quantum model\n",
        "ax = axes[model_idx]\n",
        "y_pred_quantum = svm_quantum_final.predict(K_test_final)\n",
        "cm_quantum = confusion_matrix(y_test, y_pred_quantum)\n",
        "\n",
        "sns.heatmap(cm_quantum, annot=True, fmt='d', cmap='Purples', ax=ax,\n",
        "            xticklabels=['Healthy', \"Parkinson's\"], \n",
        "            yticklabels=['Healthy', \"Parkinson's\"])\n",
        "ax.set_title('Quantum Kernel SVM', fontsize=12, fontweight='bold')\n",
        "ax.set_ylabel('True Label', fontsize=10)\n",
        "ax.set_xlabel('Predicted Label', fontsize=10)\n",
        "\n",
        "# Hide unused subplots\n",
        "for idx in range(model_idx + 1, len(axes)):\n",
        "    axes[idx].axis('off')\n",
        "\n",
        "plt.suptitle('Confusion Matrices', fontsize=16, fontweight='bold', y=0.995)\n",
        "plt.tight_layout()\n",
        "plt.savefig(f'{OUTPUT_DIR}/confusion_matrices.png', dpi=300, bbox_inches='tight')\n",
        "plt.show()\n",
        "\n",
        "print(f\"✓ Saved confusion matrices to {OUTPUT_DIR}/confusion_matrices.png\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8. Save Models and Kernel Matrices\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import joblib\n",
        "\n",
        "# Save classical models\n",
        "for model_name, model in final_classical_models.items():\n",
        "    filename = f'{OUTPUT_DIR}/model_{model_name.replace(\" \", \"_\").lower()}.pkl'\n",
        "    joblib.dump(model, filename)\n",
        "    print(f\"✓ Saved {model_name} to {filename}\")\n",
        "\n",
        "# Save quantum kernel SVM\n",
        "joblib.dump(svm_quantum_final, f'{OUTPUT_DIR}/model_quantum_kernel_svm.pkl')\n",
        "print(f\"✓ Saved Quantum Kernel SVM to {OUTPUT_DIR}/model_quantum_kernel_svm.pkl\")\n",
        "\n",
        "# Save kernel matrices\n",
        "np.save(f'{OUTPUT_DIR}/quantum_kernel_train.npy', K_train_final)\n",
        "np.save(f'{OUTPUT_DIR}/quantum_kernel_test.npy', K_test_final)\n",
        "print(f\"✓ Saved kernel matrices to {OUTPUT_DIR}/\")\n",
        "\n",
        "# Save scaler and PCA\n",
        "joblib.dump(scaler, f'{OUTPUT_DIR}/scaler.pkl')\n",
        "joblib.dump(pca, f'{OUTPUT_DIR}/pca.pkl')\n",
        "print(f\"✓ Saved preprocessing objects to {OUTPUT_DIR}/\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 9. Summary and Conclusions\n",
        "\n",
        "### Key Findings\n",
        "\n",
        "1. **Classical Models**: All three classical baselines (Logistic Regression, Random Forest, SVM-RBF) provide strong performance on this dataset.\n",
        "\n",
        "2. **Quantum Kernel SVM**: The quantum kernel approach demonstrates competitive performance, though computational cost is higher.\n",
        "\n",
        "3. **Subject-wise Splitting**: Critical for avoiding data leakage when multiple recordings come from the same subject.\n",
        "\n",
        "4. **Feature Engineering**: PCA dimensionality reduction is essential for quantum kernels due to hardware limitations.\n",
        "\n",
        "### Clinical Implications\n",
        "\n",
        "- **Limitations**: This analysis is for research purposes only. Clinical diagnosis requires:\n",
        "  - Larger, more diverse patient populations\n",
        "  - Validation across different demographics\n",
        "  - Integration with other clinical assessments\n",
        "  - Regulatory approval for diagnostic use\n",
        "\n",
        "- **Potential Applications**: \n",
        "  - Early screening tool\n",
        "  - Telemonitoring support\n",
        "  - Research into voice biomarkers\n",
        "\n",
        "### Future Work\n",
        "\n",
        "- Experiment with different quantum feature maps\n",
        "- Optimize quantum circuit depth and parameters\n",
        "- Test on larger datasets\n",
        "- Explore hybrid classical-quantum approaches\n",
        "- Investigate quantum advantage for specific feature interactions\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Appendix: Dependencies List\n",
        "\n",
        "```\n",
        "numpy>=1.21.0\n",
        "pandas>=1.3.0\n",
        "scikit-learn>=1.0.0\n",
        "matplotlib>=3.4.0\n",
        "seaborn>=0.11.0\n",
        "pennylane>=0.20.0\n",
        "scipy>=1.7.0\n",
        "joblib>=1.0.0\n",
        "tqdm>=4.62.0\n",
        "```\n",
        "\n",
        "### Optional (for IBM Quantum hardware):\n",
        "```\n",
        "qiskit>=0.34.0\n",
        "qiskit-ibm-provider>=0.5.0\n",
        "```\n",
        "\n",
        "### Installation:\n",
        "```bash\n",
        "pip install numpy pandas scikit-learn matplotlib seaborn pennylane scipy joblib tqdm\n",
        "```\n",
        "\n",
        "### For IBM Quantum Hardware:\n",
        "1. Sign up at https://quantum-computing.ibm.com/\n",
        "2. Get your API token\n",
        "3. Install: `pip install qiskit qiskit-ibm-provider`\n",
        "4. Set `USE_HARDWARE = True` and configure `IBM_DEVICE` in the configuration cell\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
